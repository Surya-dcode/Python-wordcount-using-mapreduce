{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc383c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting file1 and file2 from HarryPotter file\n",
    "import fitz\n",
    "def extract_text(pdf_path, start_page, end_page):\n",
    "    text = ''\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page_num in range(start_page - 1, end_page):\n",
    "            page = doc.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "start_page_1 = 1860        #as my birth month is 8, it is divided by 2, so that ch-4 is considered and birthday is 31.\n",
    "end_page_1 = 1869\n",
    "start_page_2 = 101           #DOB is 31 Aug, 2001, as birth year is 2001 page 101 is considered\n",
    "end_page_2 = 110  \n",
    "pdf_path = r'D:\\Hadoop\\assignment-1\\Assign_1\\HarryPotter.pdf'\n",
    "text_1 = extract_text(pdf_path, start_page_1, end_page_1)\n",
    "text_2 = extract_text(pdf_path, start_page_2, end_page_2)\n",
    "with open('file1.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(text_1)\n",
    "with open('file2.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07915da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31: 1\n",
      "32: 1\n",
      "33: 1\n",
      "34: 1\n",
      "35: 1\n",
      "36: 1\n",
      "37: 1\n",
      "a: 33\n",
      "able: 1\n",
      "about: 7\n",
      "achieved: 1\n",
      "added: 1\n",
      "address: 1\n",
      "after: 2\n",
      "again: 3\n",
      "against: 2\n",
      "age: 1\n",
      "all: 3\n",
      "allow: 1\n",
      "allowing: 2\n",
      "almost: 2\n",
      "aloud: 2\n",
      "already: 1\n",
      "am: 2\n",
      "and: 37\n",
      "anger: 1\n",
      "annoyance: 1\n",
      "answer: 2\n",
      "answering: 1\n",
      "anyone: 3\n",
      "anything: 3\n",
      "apparently: 1\n",
      "are: 2\n",
      "around: 1\n",
      "arrest: 1\n",
      "arrived: 1\n",
      "arthur: 1\n",
      "as: 16\n",
      "asked: 1\n",
      "at: 11\n",
      "aunt: 1\n",
      "back: 5\n",
      "badly: 1\n",
      "baggy: 1\n",
      "barked: 1\n",
      "battle: 1\n",
      "be: 8\n",
      "because: 1\n",
      "become: 1\n",
      "been: 4\n",
      "before: 1\n",
      "began: 1\n",
      "behind: 2\n",
      "being: 2\n",
      "best: 1\n",
      "bewildered: 1\n",
      "bit: 2\n",
      "black: 1\n",
      "blank: 1\n",
      "blotchily: 1\n",
      "both: 1\n",
      "brandished: 1\n",
      "break: 1\n",
      "breast: 1\n",
      "breath: 2\n",
      "bristled: 1\n",
      "britain: 1\n",
      "broom: 1\n",
      "broomsticks: 1\n",
      "but: 6\n",
      "by: 2\n",
      "call: 1\n",
      "calm: 1\n",
      "came: 3\n",
      "can: 4\n",
      "casually: 1\n",
      "chair: 1\n",
      "children: 1\n",
      "closed: 2\n",
      "clothes: 1\n",
      "cogs: 1\n",
      "coldly: 1\n",
      "color: 1\n",
      "com: 10\n",
      "come: 2\n",
      "conclusion: 1\n",
      "conflict: 1\n",
      "confusion: 1\n",
      "connected: 1\n",
      "connections: 1\n",
      "continued: 1\n",
      "contract: 1\n",
      "could: 6\n",
      "couldn: 2\n",
      "course: 1\n",
      "covered: 1\n",
      "cream: 1\n",
      "crossed: 1\n",
      "cup: 6\n",
      "curiously: 1\n",
      "currant: 1\n",
      "curtly: 1\n",
      "d: 1\n",
      "dark: 1\n",
      "days: 1\n",
      "deal: 1\n",
      "dear: 1\n",
      "dearly: 1\n",
      "decided: 1\n",
      "deep: 1\n",
      "delivered: 1\n",
      "department: 1\n",
      "did: 2\n",
      "didn: 4\n",
      "diet: 1\n",
      "disappear: 1\n",
      "disgusting: 1\n",
      "distantly: 1\n",
      "distaste: 1\n",
      "do: 6\n",
      "does: 1\n",
      "doesn: 1\n",
      "done: 2\n",
      "door: 3\n",
      "doorbell: 2\n",
      "down: 5\n",
      "dressed: 1\n",
      "drew: 1\n",
      "dudley: 4\n",
      "dumpy: 2\n",
      "dursley: 2\n",
      "dursleys: 3\n",
      "e: 7\n",
      "earlier: 1\n",
      "early: 1\n",
      "ears: 1\n",
      "earth: 1\n",
      "effect: 1\n",
      "else: 1\n",
      "end: 1\n",
      "enjoy: 1\n",
      "enormous: 1\n",
      "enough: 2\n",
      "envelope: 1\n",
      "especially: 1\n",
      "even: 2\n",
      "ever: 1\n",
      "every: 2\n",
      "except: 1\n",
      "expecting: 1\n",
      "express: 1\n",
      "expression: 1\n",
      "extremely: 2\n",
      "eyes: 2\n",
      "face: 7\n",
      "fear: 2\n",
      "feel: 1\n",
      "fell: 1\n",
      "felt: 1\n",
      "fight: 1\n",
      "final: 1\n",
      "finally: 2\n",
      "find: 2\n",
      "finish: 1\n",
      "finished: 2\n",
      "fire: 7\n",
      "fireplace: 1\n",
      "five: 1\n",
      "flash: 1\n",
      "flashed: 1\n",
      "followed: 1\n",
      "following: 1\n",
      "food: 1\n",
      "for: 14\n",
      "forced: 1\n",
      "form: 1\n",
      "forming: 1\n",
      "friend: 1\n",
      "from: 7\n",
      "front: 2\n",
      "frowned: 1\n",
      "fundamental: 1\n",
      "funny: 1\n",
      "furious: 1\n",
      "fuss: 1\n",
      "g: 7\n",
      "games: 1\n",
      "get: 3\n",
      "give: 1\n",
      "glad: 1\n",
      "glare: 1\n",
      "glared: 1\n",
      "glaring: 1\n",
      "glass: 1\n",
      "go: 5\n",
      "goblet: 7\n",
      "godfather: 1\n",
      "going: 4\n",
      "gone: 1\n",
      "got: 3\n",
      "grapefruit: 1\n",
      "great: 2\n",
      "gritted: 1\n",
      "growled: 2\n",
      "had: 15\n",
      "hair: 2\n",
      "hall: 2\n",
      "hand: 2\n",
      "hands: 1\n",
      "happy: 1\n",
      "hard: 1\n",
      "harry: 52\n",
      "has: 2\n",
      "hasn: 1\n",
      "hated: 1\n",
      "have: 11\n",
      "having: 1\n",
      "he: 54\n",
      "heard: 3\n",
      "heaved: 1\n",
      "held: 1\n",
      "help: 1\n",
      "her: 1\n",
      "him: 9\n",
      "himself: 2\n",
      "his: 22\n",
      "hissed: 1\n",
      "hog: 1\n",
      "hogwarts: 1\n",
      "holidays: 1\n",
      "hope: 2\n",
      "hoped: 1\n",
      "hoping: 1\n",
      "hosted: 1\n",
      "house: 2\n",
      "household: 1\n",
      "how: 2\n",
      "however: 1\n",
      "husband: 1\n",
      "i: 12\n",
      "ice: 1\n",
      "if: 6\n",
      "in: 17\n",
      "inch: 1\n",
      "increased: 1\n",
      "indeed: 1\n",
      "instincts: 1\n",
      "interested: 1\n",
      "into: 4\n",
      "introduced: 1\n",
      "is: 4\n",
      "it: 12\n",
      "j: 7\n",
      "jeans: 1\n",
      "just: 4\n",
      "k: 7\n",
      "keep: 2\n",
      "kettle: 1\n",
      "kitchen: 1\n",
      "knees: 1\n",
      "knew: 1\n",
      "know: 7\n",
      "knows: 1\n",
      "lack: 1\n",
      "large: 2\n",
      "last: 1\n",
      "laugh: 1\n",
      "laughing: 1\n",
      "let: 1\n",
      "letter: 8\n",
      "letters: 1\n",
      "lifetime: 2\n",
      "like: 3\n",
      "lips: 1\n",
      "lived: 1\n",
      "livid: 1\n",
      "living: 2\n",
      "load: 1\n",
      "long: 2\n",
      "look: 3\n",
      "looked: 6\n",
      "looking: 1\n",
      "loudly: 1\n",
      "loved: 1\n",
      "magic: 1\n",
      "magical: 1\n",
      "make: 2\n",
      "making: 2\n",
      "managed: 1\n",
      "many: 2\n",
      "marching: 1\n",
      "match: 1\n",
      "me: 1\n",
      "mean: 1\n",
      "meeting: 1\n",
      "mention: 1\n",
      "mentioned: 1\n",
      "merely: 1\n",
      "might: 4\n",
      "mind: 1\n",
      "minute: 2\n",
      "mistake: 1\n",
      "mistreated: 2\n",
      "mixed: 1\n",
      "molly: 1\n",
      "monday: 1\n",
      "morning: 1\n",
      "most: 1\n",
      "mother: 2\n",
      "mr: 1\n",
      "mrs: 6\n",
      "muggle: 1\n",
      "mustache: 2\n",
      "mustached: 1\n",
      "muttered: 1\n",
      "my: 5\n",
      "name: 1\n",
      "neatly: 1\n",
      "neighbors: 1\n",
      "nerves: 1\n",
      "nervous: 1\n",
      "neutral: 1\n",
      "never: 2\n",
      "next: 1\n",
      "night: 1\n",
      "nobody: 1\n",
      "normal: 5\n",
      "not: 7\n",
      "noticed: 1\n",
      "now: 4\n",
      "occupied: 1\n",
      "of: 37\n",
      "off: 3\n",
      "okay: 1\n",
      "on: 9\n",
      "once: 1\n",
      "one: 2\n",
      "only: 3\n",
      "onto: 1\n",
      "opportunity: 1\n",
      "or: 1\n",
      "ordinary: 1\n",
      "other: 2\n",
      "our: 1\n",
      "out: 6\n",
      "outraged: 1\n",
      "over: 1\n",
      "owl: 1\n",
      "own: 2\n",
      "p: 8\n",
      "panicky: 1\n",
      "paper: 2\n",
      "parted: 1\n",
      "past: 1\n",
      "people: 2\n",
      "perusing: 2\n",
      "petunia: 2\n",
      "piece: 1\n",
      "place: 1\n",
      "played: 1\n",
      "plum: 1\n",
      "pocket: 1\n",
      "politely: 1\n",
      "possible: 2\n",
      "post: 1\n",
      "postman: 3\n",
      "potter: 7\n",
      "pressed: 1\n",
      "prime: 1\n",
      "pronounce: 1\n",
      "pupils: 1\n",
      "purple: 3\n",
      "put: 4\n",
      "puzzled: 1\n",
      "quick: 1\n",
      "quickly: 1\n",
      "quidditch: 5\n",
      "rage: 1\n",
      "rang: 2\n",
      "re: 2\n",
      "read: 1\n",
      "reading: 1\n",
      "really: 1\n",
      "recede: 1\n",
      "red: 1\n",
      "refuge: 1\n",
      "remainder: 1\n",
      "remember: 1\n",
      "rest: 2\n",
      "rich: 2\n",
      "rid: 1\n",
      "right: 3\n",
      "ripping: 1\n",
      "roll: 1\n",
      "ron: 2\n",
      "roof: 1\n",
      "room: 3\n",
      "rowling: 7\n",
      "rubbish: 1\n",
      "ruddy: 1\n",
      "rules: 1\n",
      "s: 24\n",
      "safely: 1\n",
      "said: 17\n",
      "satisfaction: 1\n",
      "saw: 2\n",
      "say: 3\n",
      "school: 3\n",
      "scowled: 1\n",
      "screwed: 1\n",
      "second: 1\n",
      "see: 7\n",
      "seemed: 2\n",
      "seen: 2\n",
      "send: 2\n",
      "sent: 1\n",
      "set: 2\n",
      "settled: 1\n",
      "severe: 1\n",
      "shaking: 1\n",
      "sharply: 1\n",
      "she: 5\n",
      "shortly: 1\n",
      "shot: 1\n",
      "should: 1\n",
      "signature: 1\n",
      "silence: 1\n",
      "since: 2\n",
      "sincerely: 1\n",
      "single: 1\n",
      "sirius: 4\n",
      "sleeves: 1\n",
      "slight: 1\n",
      "slightly: 1\n",
      "smile: 1\n",
      "so: 6\n",
      "some: 2\n",
      "someone: 2\n",
      "something: 5\n",
      "son: 2\n",
      "soon: 1\n",
      "sort: 1\n",
      "sound: 3\n",
      "spasm: 1\n",
      "spat: 1\n",
      "spoken: 1\n",
      "sport: 1\n",
      "sports: 1\n",
      "square: 1\n",
      "squeezed: 1\n",
      "stab: 1\n",
      "stamps: 4\n",
      "stand: 3\n",
      "staring: 1\n",
      "start: 1\n",
      "stay: 1\n",
      "steadying: 1\n",
      "still: 1\n",
      "stole: 1\n",
      "stop: 3\n",
      "stopped: 1\n",
      "strain: 1\n",
      "struggled: 1\n",
      "stupid: 3\n",
      "sudden: 1\n",
      "summer: 2\n",
      "supposed: 1\n",
      "sure: 3\n",
      "swear: 1\n",
      "sweatshirt: 1\n",
      "t: 12\n",
      "table: 1\n",
      "take: 2\n",
      "takes: 1\n",
      "talking: 1\n",
      "tall: 1\n",
      "teapot: 1\n",
      "teeth: 1\n",
      "tell: 3\n",
      "temper: 2\n",
      "term: 1\n",
      "tested: 1\n",
      "than: 2\n",
      "that: 10\n",
      "the: 83\n",
      "their: 2\n",
      "them: 2\n",
      "then: 8\n",
      "there: 3\n",
      "therefore: 1\n",
      "these: 1\n",
      "they: 2\n",
      "thick: 1\n",
      "thing: 2\n",
      "think: 2\n",
      "thinking: 2\n",
      "thirteen: 1\n",
      "thirty: 1\n",
      "this: 13\n",
      "though: 5\n",
      "thought: 2\n",
      "threatening: 1\n",
      "three: 1\n",
      "through: 3\n",
      "tickets: 2\n",
      "time: 2\n",
      "times: 2\n",
      "tiny: 1\n",
      "to: 60\n",
      "told: 2\n",
      "too: 2\n",
      "took: 2\n",
      "touchy: 1\n",
      "train: 2\n",
      "transparent: 1\n",
      "treat: 1\n",
      "trembling: 1\n",
      "tried: 3\n",
      "trying: 2\n",
      "turning: 1\n",
      "two: 2\n",
      "uncle: 34\n",
      "under: 5\n",
      "understand: 1\n",
      "ungrateful: 1\n",
      "unnaturalness: 1\n",
      "unpleasant: 1\n",
      "up: 4\n",
      "us: 4\n",
      "use: 1\n",
      "uttered: 1\n",
      "vaguely: 1\n",
      "ve: 3\n",
      "vernon: 29\n",
      "very: 2\n",
      "voice: 1\n",
      "wait: 1\n",
      "waited: 1\n",
      "want: 1\n",
      "was: 21\n",
      "wasn: 3\n",
      "watched: 1\n",
      "way: 4\n",
      "we: 3\n",
      "weasley: 6\n",
      "weasleys: 1\n",
      "weeks: 1\n",
      "well: 2\n",
      "were: 5\n",
      "what: 7\n",
      "when: 3\n",
      "where: 3\n",
      "which: 3\n",
      "while: 2\n",
      "who: 6\n",
      "why: 2\n",
      "wider: 1\n",
      "will: 2\n",
      "window: 1\n",
      "with: 11\n",
      "wizards: 1\n",
      "woman: 2\n",
      "wondering: 1\n",
      "word: 2\n",
      "words: 3\n",
      "working: 1\n",
      "world: 5\n",
      "worst: 1\n",
      "would: 11\n",
      "write: 2\n",
      "writing: 5\n",
      "wrong: 1\n",
      "www: 10\n",
      "yeah: 1\n",
      "years: 2\n",
      "you: 16\n",
      "your: 3\n",
      "yours: 1\n",
      "ztcprep: 10\n"
     ]
    }
   ],
   "source": [
    "#file1 word count\n",
    "#DOB is 31 aug, 2001\n",
    "import re\n",
    "# Function to process input file and emit word count pairs for each line\n",
    "def map_func(input_file):\n",
    "    word_counts = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            wc = {}\n",
    "            for word in re.findall(r'\\b\\w+\\b', line.lower()):\n",
    "                wc[word] = wc.get(word, 0) + 1\n",
    "            word_counts.append(sorted(wc.items(), key=lambda x: x[0]))\n",
    "    return word_counts\n",
    "# Function to aggregate word counts\n",
    "def reduce_func(wc_pairs):\n",
    "    word_count = {}\n",
    "    for wc_line in wc_pairs:\n",
    "        for word, count in wc_line:\n",
    "            word_count[word] = word_count.get(word, 0) + count\n",
    "    return sorted(word_count.items(), key=lambda x: x[0])\n",
    "# Map phase\n",
    "mapped_data = map_func(\"file1.txt\")\n",
    "# Reduce phase\n",
    "reduced_data = reduce_func(mapped_data)\n",
    "# Print the word count result\n",
    "for word, count in reduced_data:\n",
    "    print(f'{word}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "021845e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albus: 2\n",
      "aren: 1\n",
      "cept: 1\n",
      "com: 10\n",
      "deliverin: 1\n",
      "diagon: 1\n",
      "didn: 6\n",
      "dudley: 7\n",
      "dumbled: 1\n",
      "dumbledore: 4\n",
      "everythin: 1\n",
      "fetchin: 1\n",
      "gettin: 2\n",
      "goin: 1\n",
      "gringotts: 5\n",
      "hadn: 3\n",
      "hagrid: 27\n",
      "hogwarts: 7\n",
      "james: 1\n",
      "knuts: 2\n",
      "ll: 7\n",
      "london: 1\n",
      "meself: 1\n",
      "mm: 1\n",
      "muggle: 1\n",
      "payin: 1\n",
      "pposed: 1\n",
      "rowling: 7\n",
      "shouldn: 1\n",
      "speakin: 1\n",
      "teh: 1\n",
      "ter: 19\n",
      "ve: 3\n",
      "vernon: 6\n",
      "wasn: 2\n",
      "wouldn: 1\n",
      "www: 10\n",
      "yeh: 10\n",
      "ztcprep: 10\n"
     ]
    }
   ],
   "source": [
    "#file 2 non english word count\n",
    "#DOB is 31 aug, 2001\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "# Create a SpellChecker object\n",
    "spell = SpellChecker()\n",
    "# Function to read input text file and emit non-English word count pairs for each line\n",
    "def mapper(input_file):\n",
    "    results = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            counts = {}\n",
    "            for word in re.findall(r'\\b\\w+\\b', line.lower()):\n",
    "                if word.lower() not in spell and not any(char.isdigit() for char in word):\n",
    "                    counts[word] = counts.get(word, 0) + 1\n",
    "            sorted_counts = sorted(counts.items(), key=lambda x: x[0])\n",
    "            results.append(sorted_counts)\n",
    "    return results\n",
    "# Function to aggregate non-English word counts\n",
    "def reducer(pairs):\n",
    "    counts = {}\n",
    "    for lines in pairs:\n",
    "        for word, count in lines:\n",
    "            counts[word] = counts.get(word, 0) + count\n",
    "    sorted_counts = sorted(counts.items(), key=lambda x: x[0])\n",
    "    return sorted_counts\n",
    "# Map phase\n",
    "mapped_data = mapper(\"file2.txt\")\n",
    "# Reduce phase\n",
    "reduced_data = reducer(mapped_data)\n",
    "# Print the non-English word count result\n",
    "for word, count in reduced_data:\n",
    "    print(f'{word}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b62267f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
